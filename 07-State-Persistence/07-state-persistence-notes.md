# Docker storage

when we install docker on a host, the docker store it's files like this

- var/lib/docker
  - aufs
  - containers
  - image
  - volumes

All files related to a container will reside inside the `containers` folder, same for `images`.

## Layered architecture
When we create a docker image, from a docker file, each instruction in docker file acts a layer, where each instruction create a layer on top of the last layer.

```Dockerfile
From Ubuntu

RUN apt-get update && apt-get -y install python

RUN pip install flask flask-mysql

COPY . /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask
```

```bash
docker build Dockerfile -t abdul/my-custom-app
```

Running above will create following layers

- Layer 1. Base Ubuntu layer (120 MB)
- Layer 2. Changes in apt packages (306 MB)
- Layer 3. Changes in pip packages (6.3 MB)
- Layer 4. Source code  (220 B)
- Layer 5. Update EntryPoint (0 B)


Now lets say we have another `app 2`, it has following docker file. 

```Dockerfile
From Ubuntu

RUN apt-get update && apt-get -y install python

RUN pip install flask flask-mysql

COPY app2.py /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app2.py flask
```
It will reuse the first 3 layers as they are same for app 1, so it will not build it and reuse it from other app. However it will build the layer 4 and 5 since source code is different.


> When we run a container based on image created using above docker file, the docker will create a new layer called `Container Layer` on top of the other layers. This layer will hold the container related data (logs etc.) and this layer will exist until the docker container is destroyed.

The contents of image layer are not editable, if we want to edit a file that belongs to the image layer, docker will first create a copy and then change/write on it. This is called `COPY-ON-WRITE`

What if we want to preserve the changes that are done in the `Container Layer` ? We need to create a volume :)


## 1. Volume drivers

```bash
docker volume create data_volume
```

the above command will create a new volume and create a directory inside

- var/lib/docker
  - volumes
     - data_volume

if we want to mount this volume to a container , we can do so using command

```bash
# we create a container of mysql and mount the directory of container i.e. /var/lib/mysql to a volume we created earlier
docker run -v data_volume:/var/lib/mysql mysql
```
Now even if the container is destroyed, the data stored by mysql database will persist, because it lives outside the container and in the host.

If the volume does not exist and we run the above command, docker will automatically create a new volume for that, for example

```bash
# data_volume2 does not exist , but docker will create it automatically
docker run -v data_volume2:/var/lib/mysql mysql
```

the above binding is called `volume mount`, i.e. we bind the volume directory on docker host to a directory of container. What if we want to bind a different directory of host instead of /var/lib/docker/volumes ?
We can do it using the `bind mount`. this allow us to mount any directory on docker host to a directory in docker, for this to work, we need to provide complete path of the directory when creating the container.

```bash
# here we provide the complete path that we need to mount i.e. /data/mysql
docker run - v /data/mysql:/var/lib/mysql mysql 
```

`-v` is the old way of doing things, new syntax allow us to provide key value pair with `--mount` attribute

```bash
docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql
```

## 2. Storage drivers
So who does all the , maintaining a layered architecture, creating writable layers, moving files across layers, enable copy-write operations ? This is done by the underlying storage driver we use. Storage drivers depend upon the underlying OS. For Ubuntu we have AUFS.
Some of the common drivers are 

- AUFS
- ZFS
- Device Mapper
- Overlay

Keep in mind the volumes are not managed by storage drivers, instead these are managed by `volume driver plugin`. The default volume driver plugin is called `local`.

Some of the popular volume driver plugins are 
- Azure file storage
- Convoy
- Flocker
- RexRay
- DigitalOcean Block Storage
- VMware vSphere Storage


# Persistent Volumes
When a pod in kubernetes is destroyed, the data generated by it also get deleted by default. If we want to persist the data, we have to configure a volume with the POD.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: random-number-generator
spec:
 containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh", "-c"]
    args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
    volumeMounts:
     - mountPath: /opt    # location of data in container
       name: data-volume  # name of volume
 volumes:
  - name: data-volume
    hostPath:
      path: /data         # path in node
      type: Directory
```

There are different volume storage options available in k8s.
- In case of a single node cluster we can persist data on the host using the `hostPath`.
- for multi node cluster, `hostPath` should not be used because every node will have it's own disk instead of having a centralized disk space.

- Some of the popular storage solutions are
  - NFS
  - GlusterFS
  - Flocker
  - Ceph
  - Amazon Elastic Block Storage (EBS)
  - Azure Disk/File storage

  For example to use AWS storage we can have

```yaml
 volumes:
  - name: data-volume
    awsElasticBlockStore:
     volumeID: <volume-id>
     fsType: ext4         # file type
  ```

Instead of creating volume in every Pod definition file, we can have a single persistent volume object and use it in several places. Also it will abstract out the details of the volume how it is implemented. 

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
 name: pv-vol1
spec:
 accessModes:
   - ReadWriteOnce
 capacity:
  storage: 1Gi
 hostPath:
  path: /tmp/data
```
For `accessModes` we have following values
- `ReadWriteOnce`: Only one node (thus pods on that node) can mount the volume for read/write at a time. Used when your application (e.g. database) needs read/write access and wonâ€™t be replicated across nodes.
- `ReadOnlyMany`: Multiple pods across different nodes can mount the volume read-only. Serving static assets (e.g., images, binaries, HTML files) to multiple frontend pods.
- `ReadWriteMany`: Multiple pods across different nodes can read and write to the same volume. Shared file systems, collaborative apps, CMS platforms, logs written by multiple pods.


# Persistent Volume Claims (PVC)
A PersistentVolumeClaim (PVC) is a request for storage by a user or application in Kubernetes. It is how a pod claims and uses a PersistentVolume (PV).

Kubernetes will try to find a volume that has the (sufficient capacity, access modes, volume modes, storage class) set in the request.

- A PV is like a physical disk or network file system.
- A PVC is like asking for a disk of a certain size and access mode (RWO, ROX, RWX).
- The pod uses the PVC to mount the underlying volume

> If no volume is available that matches the request specs, then the claim will remain in `pending` state

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: myclaim
spec:
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 500Mi
```

if we create the above mentioned volume `pv-vol1` and then create the pvc `myclaim`, this claim will bound to the `pv-vol1` volume because requested storage is 500Mi whereas the volume support up to 1Gi.


## What if there are multiple volumes that matches the given specification in request ?
  - We can set labels in the volume and add the `selector` with `matchLabels` in the PVC.

```yaml
labels:
 name: my-pv
```
```yaml
selector:
 matchLabels:
  name: my-pv
```

## What if the claim is smaller and the volume available is larger and no other volume is available that matches the request ?
- kubernetes will bind the larger volume to the smaller claim
- There is one to one relationship between claims and volumes, hence once occupied the volume remaining capacity cannot be used by other claims.


## What will happen if we delete the claim ?
 - By default the underlying volume will not be deleted because by default the `persistentVolumeReclaimPolicy` is set to `Retain`. The administrator can manually delete the volume. And it cannot be used by any other claims.
- if we set `persistentVolumeReclaimPolicy` to `Delete` then the volume will be deleted once the claim is deleted.
- The third option is `Recycle`, this will scrub the data on volume when the claim is deleted, hence making the volume available to be used by other claims. 

## How to configure claim on a Pod, Deployment and Replica set ?

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: mypod
spec:
 containers:
  -name: frontend
   image: nginx
   volumeMounts:
    - mountPath: "/var/www/html"
      name: mypd

 volumes:
   - name: mypd
     persistentVolumeClaim:
      claimName: myclaim
```


# Storage Classes

## Static Provisioning
Lets say we want to create a volume in google cloud, we first need to provision a disk, then we can create a volume, this is called static provisioning. 

```bash
gcloud beta compute disks create --size 1GB --region us-east1
```

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
 name: pv-1
spec:
 accessMode:
  - ReadWriteOnce
 capacity:
  storage: 500Mi
 gcePersistentDisk:
  pdName: pd-disk
  fsType: ext4
```

## Dynamic Provisioning
How about the volume is created automatically when the pod needs the storage, in these kind of scenarios we need storage classes. When we create a storage class, we no longer need to create volumes, we just create the claims ,and these claims can automatically get their desired capacity from the storage class.
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: google-storage
provisioner: kubernetes.io/gce-pd
```
then we mention the name of storage class in claim

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: my-claim
spec:
 storageClassName: google-storage 
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 500Mi
```
this will create a `PersistentVolume` behind the scenes and when a Pod requests a storage via claim ,this new volume will be used.

We can have multiple storage classes, for example one using standard storage, other SSD and another for network file sharing. This will allow us to claim storage from different classes.

# Stateful Set

## Problem statement
Let's say we have a scenario, where we want to deploy highly available MySql database. With following requirements

 - Deployment should have a master and 2 slaves
 - Slave 1 should copy the initial data from master, and then point to master for replication
 - Slave 2 should copy the initial data from slave-1 and then point to master for replication

## Challenges
 - Since k8s assign dynamic IP's to every new Pod, we cannot use IP address to setup the communication between master and slaves.
 - Also k8s assign random names to Pod, we also cannot use that
 - Static host names should be there in order to setup the communication
 - Pods should be created in order 
  - Master should be there first, to first apply initial data dump
  - Slave-1 should be next, so that it can be copy the initial data from master and then point for replication to master
  - Similarly slave-2 should be initialized at the end

 ## Characteristics
  In such cases when we want an ordered deployment along with predicatable names for the Pods, like `mysql-0`,`mysql-1` and `mysql-2` we can acheive this using stateful set. 
   - It is built for stateful apps like databases, kafka etc.
   - It has a sticky nature, even if the pod restarts, it will still have the original name.
   - Every pod in stateful set can have it own persistent volume claim (PVC)
   - Pods are created one by one (mysql-0 first, then mysql-1, etc.).
   - Updates happen in sequence, ensuring the app stays consistent.

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
 name: mysql
 labels:
  app: mysql
spec:
 replicas: 3
 serviceName: mysql-h            # this must be a headless service
 podManagementPolicy: Parallel   # by default pods are created in order, we can change it to Parallel
 selector:
  matchLabels:
   app: mysql
 template:
  metadata:
   labels:
    app: mysql
  spec:
   containers:
    - image: mysql
      name: mysql
```

## Headless Service
Since in initial requirements it was needed that writes should only go to the master pod and reads can be done from master and slaves also.
How can we acheive this ? Since a regular service acts as a load balancer and distributes the traffice among all pods!.

**We need a headless service**
A headless service creates DNS entries for all underlying pods and allow us to access them using the DNS record

A DNS record for all underlying pods will have following structure.

`podname`.`headless-servicename`.`namespace`.`svc`.`cluster-domain`.`example`

e.g
1. mysql-0.mysql-h.default.svc.cluster.local
2. mysql-1.mysql-h.default.svc.cluster.local
3. mysql-2.mysql-h.default.svc.cluster.local


```yaml
apiVersion: v1
kind: Service
metadata:
 name: mysql-h
spec:
 clusterIP: None  #this setting make it a headless service
 ports:
  - port: 3306
 selector:
  app: mysql
```

There are two properties in a `Deployment` under the spec section which can be used to set the domain and subdomain for the pods.

```yaml
spec:
 subdomain: mysql-h
 hostname: mysql-pod
```
These two properties should be set at the Pod level in order to generate `A` DNS record for the Pods.**But this will generate the A records with same address**

`mysql-pod.mysql-h.default.svc.cluster.local`

That's another difference between a `Deployment` and `StatefulSet`, you don't need to set he `subdomain` and `hostname`, instead you specify the headless service under the spec => `service` property that will generate the A records for all the underlying pods. 

So for stateful set
- Pod Name will act as ==> hostname
- Service Name will act as  ==> subdomain


## Storage in Stateful Set
if we want to use the same volume for all the pods in a deployment or stateful set, then we can just use `volumes` property. If we want every pod should have a sepearte volume, then we will use `volumeClaimTemplate`


```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
 name: mysql
 labels:
  app: mysql
spec:
 replicas: 3
 serviceName: mysql-h            # this must be a headless service
 selector:
  matchLabels:
   app: mysql
 template:
  metadata:
   labels:
    app: mysql
  spec:
   containers:
    - image: mysql
      name: mysql
 volumeClaimTemplate:
  - metadata:
     name: data-volume
    spec:
     accessModes:
      - ReadWriteOnce
     storageClassName: google-storage
     resources:
      requests:
       storage: 500Mi
```