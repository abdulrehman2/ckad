# Docker storage

when we install docker on a host, the docker store it's files like this

- var/lib/docker
  - aufs
  - containers
  - image
  - volumes

All files related to a container will reside inside the `containers` folder, same for `images`.

## Layered architecture
When we create a docker image, from a docker file, each instruction in docker file acts a layer, where each instruction create a layer on top of the last layer.

```Dockerfile
From Ubuntu

RUN apt-get update && apt-get -y install python

RUN pip install flask flask-mysql

COPY . /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask
```

```bash
docker build Dockerfile -t abdul/my-custom-app
```

Running above will create following layers

- Layer 1. Base Ubuntu layer (120 MB)
- Layer 2. Changes in apt packages (306 MB)
- Layer 3. Changes in pip packages (6.3 MB)
- Layer 4. Source code  (220 B)
- Layer 5. Update EntryPoint (0 B)


Now lets say we have another `app 2`, it has following docker file. 

```Dockerfile
From Ubuntu

RUN apt-get update && apt-get -y install python

RUN pip install flask flask-mysql

COPY app2.py /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app2.py flask
```
It will reuse the first 3 layers as they are same for app 1, so it will not build it and reuse it from other app. However it will build the layer 4 and 5 since source code is different.


> When we run a container based on image created using above docker file, the docker will create a new layer called `Container Layer` on top of the other layers. This layer will hold the container related data (logs etc.) and this layer will exist until the docker container is destroyed.

The contents of image layer are not editable, if we want to edit a file that belongs to the image layer, docker will first create a copy and then change/write on it. This is called `COPY-ON-WRITE`

What if we want to preserve the changes that are done in the `Container Layer` ? We need to create a volume :)


## 1. Volume drivers

```bash
docker volume create data_volume
```

the above command will create a new volume and create a directory inside

- var/lib/docker
  - volumes
     - data_volume

if we want to mount this volume to a container , we can do so using command

```bash
# we create a container of mysql and mount the directory of container i.e. /var/lib/mysql to a volume we created earlier
docker run -v data_volume:/var/lib/mysql mysql
```
Now even if the container is destroyed, the data stored by mysql database will persist, because it lives outside the container and in the host.

If the volume does not exist and we run the above command, docker will automatically create a new volume for that, for example

```bash
# data_volume2 does not exist , but docker will create it automatically
docker run -v data_volume2:/var/lib/mysql mysql
```

the above binding is called `volume mount`, i.e. we bind the volume directory on docker host to a directory of container. What if we want to bind a different directory of host instead of /var/lib/docker/volumes ?
We can do it using the `bind mount`. this allow us to mount any directory on docker host to a directory in docker, for this to work, we need to provide complete path of the directory when creating the container.

```bash
# here we provide the complete path that we need to mount i.e. /data/mysql
docker run - v /data/mysql:/var/lib/mysql mysql 
```

`-v` is the old way of doing things, new syntax allow us to provide key value pair with `--mount` attribute

```bash
docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql
```

## 2. Storage drivers
So who does all the , maintaining a layered architecture, creating writable layers, moving files across layers, enable copy-write operations ? This is done by the underlying storage driver we use. Storage drivers depend upon the underlying OS. For Ubuntu we have AUFS.
Some of the common drivers are 

- AUFS
- ZFS
- Device Mapper
- Overlay

Keep in mind the volumes are not managed by storage drivers, instead these are managed by `volume driver plugin`. The default volume driver plugin is called `local`.

Some of the popular volume driver plugins are 
- Azure file storage
- Convoy
- Flocker
- RexRay
- DigitalOcean Block Storage
- VMware vSphere Storage


# Persistent Volumes
When a pod in kubernetes is destroyed, the data generated by it also get deleted by default. If we want to persist the data, we have to configure a volume with the POD.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: random-number-generator
spec:
 containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh", "-c"]
    args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
    volumeMounts:
     - mountPath: /opt    # location of data in container
       name: data-volume  # name of volume
 volumes:
  - name: data-volume
    hostPath:
      path: /data         # path in node
      type: Directory
```

There are different volume storage options available in k8s.
- In case of a single node cluster we can persist data on the host using the `hostPath`.
- for multi node cluster, `hostPath` should not be used because every node will have it's own disk instead of having a centralized disk space.

- Some of the popular storage solutions are
  - NFS
  - GlusterFS
  - Flocker
  - Ceph
  - Amazon Elastic Block Storage (EBS)
  - Azure Disk/File storage

  For example to use AWS storage we can have

```yaml
 volumes:
  - name: data-volume
    awsElasticBlockStore:
     volumeID: <volume-id>
     fsType: ext4         # file type
  ```

Instead of creating volume in every Pod definition file, we can have a single persistent volume object and use it in several places. Also it will abstract out the details of the volume how it is implemented. 

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
 name: pv-vol1
spec:
 accessModes:
   - ReadWriteOnce
 capacity:
  storage: 1Gi
 hostPath:
  path: /tmp/data
```
For `accessModes` we have following values
- `ReadWriteOnce`: Only one node (thus pods on that node) can mount the volume for read/write at a time. Used when your application (e.g. database) needs read/write access and wonâ€™t be replicated across nodes.
- `ReadOnlyMany`: Multiple pods across different nodes can mount the volume read-only. Serving static assets (e.g., images, binaries, HTML files) to multiple frontend pods.
- `ReadWriteMany`: Multiple pods across different nodes can read and write to the same volume. Shared file systems, collaborative apps, CMS platforms, logs written by multiple pods.


# Persistent Volume Claims (PVC)
A PersistentVolumeClaim (PVC) is a request for storage by a user or application in Kubernetes. It is how a pod claims and uses a PersistentVolume (PV).

Kubernetes will try to find a volume that has the (sufficient capacity, access modes, volume modes, storage class) set in the request.

- A PV is like a physical disk or network file system.
- A PVC is like asking for a disk of a certain size and access mode (RWO, ROX, RWX).
- The pod uses the PVC to mount the underlying volume

> If no volume is available that matches the request specs, then the claim will remain in `pending` state

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: myclaim
spec:
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 500Mi
```

if we create the above mentioned volume `pv-vol1` and then create the pvc `myclaim`, this claim will bound to the `pv-vol1` volume because requested storage is 500Mi whereas the volume support up to 1Gi.


## What if there are multiple volumes that matches the given specification in request ?
  - We can set labels in the volume and add the `selector` with `matchLabels` in the PVC.

```yaml
labels:
 name: my-pv
```
```yaml
selector:
 matchLabels:
  name: my-pv
```

## What if the claim is smaller and the volume available is larger and no other volume is available that matches the request ?
- kubernetes will bind the larger volume to the smaller claim
- There is one to one relationship between claims and volumes, hence once occupied the volume remaining capacity cannot be used by other claims.


## What will happen if we delete the claim ?
 - By default the underlying volume will not be deleted because by default the `persistentVolumeReclaimPolicy` is set to `Retain`. The administrator can manually delete the volume. And it cannot be used by any other claims.
- if we set `persistentVolumeReclaimPolicy` to `Delete` then the volume will be deleted once the claim is deleted.
- The third option is `Recycle`, this will scrub the data on volume when the claim is deleted, hence making the volume available to be used by other claims. 

## How to configure claim on a Pod, Deployment and Replica set ?

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: mypod
spec:
 containers:
  -name: frontend
   image: nginx
   volumeMounts:
    - mountPath: "/var/www/html"
      name: mypd

 volumes:
   - name: mypd
     persistentVolumeClaim:
      claimName: myclaim
```


# Storage Classes

## Static Provisioning
Lets say we want to create a volume in google cloud, we first need to provision a disk, then we can create a volume, this is called static provisioning. 

```bash
gcloud beta compute disks create --size 1GB --region us-east1
```

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
 name: pv-1
spec:
 accessMode:
  - ReadWriteOnce
 capacity:
  storage: 500Mi
 gcePersistentDisk:
  pdName: pd-disk
  fsType: ext4
```

## Dynamic Provisioning
How about the volume is created automatically when the pod needs the storage, in these kind of scenarios we need storage classes. When we create a storage class, we no longer need to create volumes, we just create the claims ,and these claims can automatically get their desired capacity from the storage class.
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: google-storage
provisioner: kubernetes.io/gce-pd

then we mention the name of storage class in claim

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: my-claim
spec:
 storageClassName: google-storage 
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 500Mi
```
this will create a `PersistentVolume` behind the scenes and when a Pod requests a storage via claim ,this new volume will be used.

We can have multiple storage classes, for example one using standard storage, other SSD and another for network file sharing. This will allow us to claim storage from different classes.


